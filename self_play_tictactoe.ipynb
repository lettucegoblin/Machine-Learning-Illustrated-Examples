{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in ./miniconda3/envs/llm/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: ipywidgets in ./miniconda3/envs/llm/lib/python3.12/site-packages (8.1.2)\n",
      "Requirement already satisfied: comm>=0.1.3 in ./miniconda3/envs/llm/lib/python3.12/site-packages (from ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in ./miniconda3/envs/llm/lib/python3.12/site-packages (from ipywidgets) (8.20.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in ./miniconda3/envs/llm/lib/python3.12/site-packages (from ipywidgets) (5.7.1)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.10 in ./miniconda3/envs/llm/lib/python3.12/site-packages (from ipywidgets) (4.0.10)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.10 in ./miniconda3/envs/llm/lib/python3.12/site-packages (from ipywidgets) (3.0.10)\n",
      "Requirement already satisfied: decorator in ./miniconda3/envs/llm/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in ./miniconda3/envs/llm/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: matplotlib-inline in ./miniconda3/envs/llm/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in ./miniconda3/envs/llm/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./miniconda3/envs/llm/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (2.15.1)\n",
      "Requirement already satisfied: stack-data in ./miniconda3/envs/llm/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in ./miniconda3/envs/llm/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in ./miniconda3/envs/llm/lib/python3.12/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./miniconda3/envs/llm/lib/python3.12/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in ./miniconda3/envs/llm/lib/python3.12/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: executing in ./miniconda3/envs/llm/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: asttokens in ./miniconda3/envs/llm/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in ./miniconda3/envs/llm/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six in ./miniconda3/envs/llm/lib/python3.12/site-packages (from asttokens->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ipywidgets as widgets \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        self.board = np.zeros((3, 3), dtype=int)  # 0: empty, 1: player 1, -1: player 2\n",
    "        self.current_player = 1  # Player 1 starts\n",
    "        self.game_over = False\n",
    "        self.winner = None\n",
    "        self.history = []\n",
    "\n",
    "    def reset(self):\n",
    "        self.board.fill(0)\n",
    "        self.current_player = 1\n",
    "        self.game_over = False\n",
    "        self.winner = None\n",
    "        self.history = []\n",
    "        return self.board.copy()\n",
    "\n",
    "    def step(self, action):\n",
    "        row, col = divmod(action, 3)\n",
    "        if self.board[row, col] == 0 and not self.game_over:\n",
    "            hist = {\n",
    "                \"player\": self.current_player,\n",
    "                \"action\": action,\n",
    "                \"state\": self.board.copy()\n",
    "            }\n",
    "            \n",
    "            self.board[row, col] = self.current_player\n",
    "            hist[\"next_state\"] = self.board.copy()\n",
    "            self.history.append(hist)\n",
    "            self.check_game_over(row, col)\n",
    "            if not self.game_over:\n",
    "                self.current_player *= -1  # Switch player\n",
    "            return self.board.copy(), self.calculate_reward(), self.game_over, {}\n",
    "        return self.board.copy(), 0, self.game_over, {\"message\": \"Invalid move\"}\n",
    "\n",
    "    def calculate_reward(self):\n",
    "        if self.game_over:\n",
    "            if self.winner is not None:\n",
    "                return 1 if self.winner == 1 else -1\n",
    "            else:\n",
    "                return 0.5  # Consider a draw as a neutral outcome\n",
    "        return 0  # No reward if the game is not over\n",
    "\n",
    "    def check_game_over(self, row, col):\n",
    "        # Check for win conditions: rows, columns, diagonals\n",
    "        if np.all(self.board[row, :] == self.current_player) or \\\n",
    "           np.all(self.board[:, col] == self.current_player) or \\\n",
    "           (row == col and np.all(np.diag(self.board) == self.current_player)) or \\\n",
    "           (row + col == 2 and np.all(np.diag(np.fliplr(self.board)) == self.current_player)):\n",
    "            self.game_over = True\n",
    "            self.winner = self.current_player\n",
    "        elif not np.any(self.board == 0):\n",
    "            self.game_over = True  # Draw\n",
    "\n",
    "    def available_moves(self):\n",
    "        return [i * 3 + j for i, j in zip(*np.where(self.board == 0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, learning_rate=0.1, discount_factor=0.95, exploration_rate=1.0, exploration_decay=0.99):\n",
    "        self.q_table = {}  # Initialize Q-table with an empty dictionary\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.exploration_decay = exploration_decay\n",
    "\n",
    "    def get_q_values(self, state):\n",
    "        # Convert state to a tuple to use as a dictionary key\n",
    "        state_key = tuple(state.reshape(-1))\n",
    "        if state_key not in self.q_table:\n",
    "            self.q_table[state_key] = np.zeros(9)\n",
    "        return self.q_table[state_key]\n",
    "\n",
    "    def update_q_values(self, state, action, reward, next_state):\n",
    "        # Basic Q-learning formula to update Q-values\n",
    "        \n",
    "        current_q = self.get_q_values(state)[action]\n",
    "        max_future_q = np.max(self.get_q_values(next_state))\n",
    "        new_q = (1 - self.learning_rate) * current_q + self.learning_rate * (reward + self.discount_factor * max_future_q)\n",
    "        self.q_table[tuple(state.reshape(-1))][action] = new_q\n",
    "\n",
    "    def choose_action(self, state, available_actions):\n",
    "        if random.uniform(0, 1) < self.exploration_rate:\n",
    "            return random.choice(available_actions)  # Explore\n",
    "        else:\n",
    "            q_values = self.get_q_values(state)\n",
    "            # Filter q_values by available actions\n",
    "            q_values_filtered = np.array([q_values[i] if i in available_actions else -np.inf for i in range(9)])\n",
    "            return np.argmax(q_values_filtered)  # Exploit\n",
    "\n",
    "    def decay_exploration(self):\n",
    "        self.exploration_rate *= self.exploration_decay\n",
    "        self.exploration_rate = max(self.exploration_rate, 0.1)  # Ensure some level of exploration continues\n",
    "\n",
    "    \n",
    "def assign_p_per_agent(game, player1, player2):\n",
    "    for history in game.history:\n",
    "        action, state, next_state, player = history[\"action\"], history[\"state\"], history[\"next_state\"], history[\"player\"]\n",
    "        \n",
    "        if player == 1:\n",
    "            if game.winner == 1:\n",
    "                player1.update_q_values(state, action, 1, next_state)\n",
    "            elif game.winner == -1:\n",
    "                player1.update_q_values(state, action, -1, next_state)\n",
    "            else:\n",
    "                player1.update_q_values(state, action, 0.1, next_state)\n",
    "        else:\n",
    "            if game.winner == -1:\n",
    "                player2.update_q_values(state, action, 1, next_state)\n",
    "            elif game.winner == 1:\n",
    "                player2.update_q_values(state, action, -1, next_state)\n",
    "            else:\n",
    "                player2.update_q_values(state, action, 0.1, next_state)\n",
    "# Updating the QLearningAgent's train_q_learning function to accommodate the TicTacToe class updates\n",
    "def train_q_learning(agent, num_episodes=1000):\n",
    "    results = {\"wins\": 0, \"losses\": 0, \"draws\": 0}\n",
    "    game = TicTacToe()\n",
    "    for episode in range(num_episodes):\n",
    "        state = game.reset()\n",
    "        done = False\n",
    "        states = []\n",
    "        actions = []\n",
    "        while not done:\n",
    "            available_actions = game.available_moves()\n",
    "            action = agent.choose_action(state, available_actions)\n",
    "            next_state, reward, done, info = game.step(action)\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            state = next_state\n",
    "\n",
    "        # Update Q-values after the game is over\n",
    "        final_reward = reward\n",
    "        assign_p_per_agent(game, agent, agent)\n",
    "\n",
    "        # Update exploration rate\n",
    "        agent.decay_exploration()\n",
    "        \n",
    "        # Log results\n",
    "        if game.winner == 1:\n",
    "            results[\"wins\"] += 1\n",
    "        elif game.winner == -1:\n",
    "            results[\"losses\"] += 1\n",
    "        else:\n",
    "            results[\"draws\"] += 1\n",
    "\n",
    "\n",
    "    print(f\"Training completed. Results: {results} {agent.exploration_rate}\")\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train multiple battle royal style, return the best agent\n",
    "def train_q_learning_battle_royal(num_agents=10, num_episodes=1000):\n",
    "    # randomize the parameters of each agent\n",
    "    agents = [QLearningAgent(learning_rate=random.uniform(0.1, 0.5), discount_factor=random.uniform(0.9, 0.99), exploration_rate=random.uniform(0.5, 1.0), exploration_decay=random.uniform(0.95, 0.99)) for _ in range(num_agents)]\n",
    "    # add random agent\n",
    "    #agents.append(QLearningAgent(exploration_decay=0.0))\n",
    "    best_agent = None\n",
    "    agent_wins = [0] * num_agents\n",
    "    game = TicTacToe()\n",
    "    for episode in range(num_episodes):\n",
    "        for i, agent in enumerate(agents):\n",
    "            # match up each agent with each other\n",
    "            for j, opponent in enumerate(agents):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                \n",
    "                state = game.reset()\n",
    "                done = False\n",
    "                states = []\n",
    "                actions = []\n",
    "                movenum = 0\n",
    "                while not done:\n",
    "                    available_actions = game.available_moves()\n",
    "                    if movenum % 2 == 0: # opponent goes first\n",
    "                        action = opponent.choose_action(state, available_actions)\n",
    "                    else:\n",
    "                        action = agent.choose_action(state, available_actions)\n",
    "                    next_state, reward, done, info = game.step(action)\n",
    "                    states.append(state)\n",
    "                    actions.append(action)\n",
    "                    state = next_state\n",
    "                    movenum += 1\n",
    "                count = 0\n",
    "                for state, action in zip(states, actions):\n",
    "                    if count % 2 == 0: # Player 1(opponent)\n",
    "                        if game.winner == 1: # Player 1 won\n",
    "                            opponent.update_q_values(state, action, 1, next_state)\n",
    "                        elif game.winner == 0:\n",
    "                            opponent.update_q_values(state, action, 0.5, next_state)\n",
    "                        elif game.winner == -1: # Player 2 won so player 1 lost\n",
    "                            opponent.update_q_values(state, action, -1, next_state)\n",
    "                    else: # Player 2(agent)\n",
    "                        if game.winner == -1:\n",
    "                            agent.update_q_values(state, action, 1, next_state)\n",
    "                        elif game.winner == 0:\n",
    "                            agent.update_q_values(state, action, 0.5, next_state)\n",
    "                        elif game.winner == 1: # Player 1 won so player 2 lost\n",
    "                            agent.update_q_values(state, action, -1, next_state)\n",
    "\n",
    "                    count += 1\n",
    "                if game.winner == 1:\n",
    "                    agent_wins[j] += 1\n",
    "                    agent_wins[i] -= 1\n",
    "                elif game.winner == -1:\n",
    "                    agent_wins[i] += 1\n",
    "                    agent_wins[j] -= 1\n",
    "                else:\n",
    "                    agent_wins[i] += 0.5\n",
    "                    agent_wins[j] += 0.5\n",
    "    \n",
    "    best_agent = agents[np.argmax(agent_wins)]\n",
    "    print(f\"Training completed. Results: {agent_wins} {best_agent.exploration_rate}\")\n",
    "    # other wins\n",
    "    print(agent_wins)\n",
    "    # set best agent's exploration rate to 0.0\n",
    "    best_agent.exploration_rate = 0.0\n",
    "    return best_agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Results: [23.0, 223.0] 0.5770343067682315\n",
      "[23.0, 223.0]\n"
     ]
    }
   ],
   "source": [
    "battle_royal_agent = train_q_learning_battle_royal(num_agents=2, num_episodes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Results: {'wins': 1752, 'losses': 924, 'draws': 7324} 0.1\n"
     ]
    }
   ],
   "source": [
    "game = TicTacToe()\n",
    "agent = QLearningAgent()\n",
    "\n",
    "# Train the agent\n",
    "solo_trained_agent = train_q_learning(agent, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action_q_learning(game, available_actions):\n",
    "    state = game.board.copy()\n",
    "    print(f\"Board: {state}\")\n",
    "    \n",
    "    agent = dropdown_dict[agent_dropdown.value]\n",
    "    print(f\"Agent: {agent_dropdown.value}\")\n",
    "    action = agent.choose_action(state, available_actions)\n",
    "    row, col = divmod(action, 3)\n",
    "    #print board\n",
    "    \n",
    "    print(f\"Q-learning agent chooses action: {row}, {col}\")\n",
    "    return row, col\n",
    "\n",
    "def random_action(game, available_actions):\n",
    "    action = random.choice(available_actions)\n",
    "    row, col = divmod(action, 3)\n",
    "    print(f\"Random agent chooses action: {row}, {col}\")\n",
    "    return row, col\n",
    "\n",
    "random_actor = {\"name\": \"Random\", \"choose_action\": random_action}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad8c4d463ee749ea8558d316145a8314",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Tic-Tac-Toe: 0 wins, 0 losses'), GridBox(children=(Button(layout=Layout(height='45…"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Board: [[1 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n",
      "Agent: solo trained\n",
      "Q-learning agent chooses action: 1, 1\n",
      "Board: [[ 1  1  0]\n",
      " [ 0 -1  0]\n",
      " [ 0  0  0]]\n",
      "Agent: solo trained\n",
      "Q-learning agent chooses action: 0, 2\n",
      "Board: [[ 1  1 -1]\n",
      " [ 1 -1  0]\n",
      " [ 0  0  0]]\n",
      "Agent: solo trained\n",
      "Q-learning agent chooses action: 2, 2\n",
      "Board: [[ 1  1 -1]\n",
      " [ 1 -1  1]\n",
      " [ 0  0 -1]]\n",
      "Agent: solo trained\n",
      "Q-learning agent chooses action: 2, 0\n",
      "Player -1 wins!\n",
      "Resetting game...0 wins, 1 losses\n"
     ]
    }
   ],
   "source": [
    "wins = 0\n",
    "losses = 0\n",
    "import time\n",
    "\n",
    "# Play against the Q-learning agent with jupyter widgets\n",
    "from ipywidgets import GridBox, Button, Layout, ButtonStyle\n",
    "def play_game_q_learning():\n",
    "    game.reset()\n",
    "    buttons = [Button(layout=Layout(width=\"45px\", height=\"45px\")) for _ in range(9)]\n",
    "    grid = GridBox(buttons, layout=Layout(\n",
    "    \n",
    "        grid_template_columns=\"repeat(3, 50px)\",\n",
    "        grid_template_rows=\"repeat(3, 50px)\"\n",
    "    ))\n",
    "\n",
    "    def on_button_clicked(button, row, col):\n",
    "        make_move(row, col)\n",
    "        if not game.game_over:\n",
    "            available_actions = game.available_moves()\n",
    "            row, col = choose_action_q_learning(game, available_actions)\n",
    "            make_move(row, col)\n",
    "        if game.game_over:\n",
    "            \n",
    "            reset()\n",
    "        \n",
    "    def make_move(row, col):\n",
    "        global wins, losses\n",
    "        if game.board[row, col] == 0 and not game.game_over:\n",
    "            game.board[row, col] = game.current_player\n",
    "            buttons[row * 3 + col].description = \"X\" if game.current_player == 1 else \"O\"\n",
    "            game.check_game_over(row, col)\n",
    "            if game.game_over:\n",
    "                if game.winner is not None:\n",
    "                    print(f\"Player {game.winner} wins!\")\n",
    "                    if game.winner == 1:\n",
    "                        wins += 1\n",
    "                    else:\n",
    "                        losses += 1\n",
    "                else:\n",
    "                    print(\"It's a draw!\")\n",
    "            else:\n",
    "                game.current_player *= -1\n",
    "    def reset():\n",
    "        global label\n",
    "        print(f\"Resetting game...{wins} wins, {losses} losses\")\n",
    "        label.value = f\"Player {game.winner} wins!\" if game.winner is not None else \"It's a draw!\"\n",
    "        time.sleep(1)\n",
    "        for i, button in enumerate(buttons):\n",
    "            button.description = \" \" \n",
    "        label.value = f\"Tic-Tac-Toe: {wins} wins, {losses} losses\"\n",
    "        game.reset()\n",
    "    \n",
    "    for i, button in enumerate(buttons):\n",
    "        row, col = divmod(i, 3)\n",
    "        button.on_click(lambda _, row=row, col=col: on_button_clicked(_, row, col))\n",
    "    return grid\n",
    "\n",
    "game.reset()\n",
    "label = widgets.Label(f\"Tic-Tac-Toe: {wins} wins, {losses} losses\")\n",
    "dropdown_dict = {\"battle royal trained\": battle_royal_agent, \"solo trained\": solo_trained_agent, \"random\": random_actor}\n",
    "agent_dropdown = widgets.Dropdown(\n",
    "    options=[\"battle royal trained\", \"solo trained\", \"random\"],\n",
    "    value=\"solo trained\",\n",
    "    description='Agent:',\n",
    "    disabled=False,\n",
    ")\n",
    "checkbox = widgets.Checkbox(description=\"Allow learning\", value=False)\n",
    "footer = widgets.HBox([agent_dropdown, checkbox])\n",
    "widgets.VBox([\n",
    "    label,\n",
    "    play_game_q_learning(),\n",
    "    footer],\n",
    "    layout=widgets.Layout(align_items=\"center\")\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:llm]",
   "language": "python",
   "name": "conda-env-llm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
